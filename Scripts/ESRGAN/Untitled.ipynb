{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f958404-5f10-4772-b2a7-f4da6bcd7430",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0f14d73aa1a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import Mish\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.utils import save_image\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations import (\n",
    "    Compose, Resize, Normalize\n",
    ")\n",
    "from albumentations.pytorch import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ca1e0-c3b1-43db-9e32-a6dfef110f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938f0bb-bd0f-430f-a173-0732326e90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.vgg19_54(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdcc4c-d7db-4810-af31-3bc288651d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, res_scale=0.2, use_LeakyReLU_Mish=True):\n",
    "        super(DenseResidualBlock, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.MISH = Mish()\n",
    "\n",
    "        def block(in_features, non_linearity=True, use_LeakyReLU_Mish=True):\n",
    "            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n",
    "            if non_linearity:\n",
    "                if use_LeakyReLU_Mish:\n",
    "                    layers += [nn.LeakyReLU()]\n",
    "                else:\n",
    "                    layers += [Mish()]\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.b1 = block(in_features=1 * filters, use_LeakyReLU_Mish=use_LeakyReLU_Mish)\n",
    "        self.b2 = block(in_features=2 * filters, use_LeakyReLU_Mish=use_LeakyReLU_Mish)\n",
    "        self.b3 = block(in_features=3 * filters, use_LeakyReLU_Mish=use_LeakyReLU_Mish)\n",
    "        self.b4 = block(in_features=4 * filters, use_LeakyReLU_Mish=use_LeakyReLU_Mish)\n",
    "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
    "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        for block in self.blocks:\n",
    "            out = block(inputs)\n",
    "            inputs = torch.cat([inputs, out], 1)\n",
    "        return out.mul(self.res_scale) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdec05-c287-40b7-b916-e72cc901a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualInResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, filters, res_scale=0.2, use_LeakyReLU_Mish=True):\n",
    "        super(ResidualInResidualDenseBlock, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.dense_blocks = nn.Sequential(\n",
    "            DenseResidualBlock(filters, use_LeakyReLU_Mish),\n",
    "            DenseResidualBlock(filters, use_LeakyReLU_Mish),\n",
    "            DenseResidualBlock(filters, use_LeakyReLU_Mish)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense_blocks(x).mul(self.res_scale) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86b582-2efb-4a49-94e2-930e2554acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorRRDB(nn.Module):\n",
    "    def __init__(self, channels, filters=64, num_res_blocks=16, num_upsample=2, use_LeakyReLU_Mish=True):\n",
    "        super(GeneratorRRDB, self).__init__()\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters, use_LeakyReLU_Mish) for _ in range(num_res_blocks)])\n",
    "        # Second conv layer post residual blocks\n",
    "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
    "        # Upsampling layers\n",
    "        upsample_layers = []\n",
    "        for _ in range(num_upsample):\n",
    "            upsample_layers += [\n",
    "                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsample_layers)\n",
    "        # Final output block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80885b-f385-46ac-8030-df2af0a38a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape, use_LeakyReLU_Mish=True):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "        self.MISH = Mish()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1)]\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            if use_LeakyReLU_Mish:\n",
    "                layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                layers.append(self.MISH)\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            if use_LeakyReLU_Mish:\n",
    "                layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                layers.append(self.MISH)\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfdfb7-578c-4b57-a10e-67e93f95383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_loop(metaclass=ABCMeta):\n",
    "    def __init__(self, epoch=0, n_epochs=200, dataset_name=\"../dataset/TurkishPlates\", batch_size=3, lr=0.0002, b1=0.9,\n",
    "                 b2=0.999, decay_epoch=100, n_cpu=8, hr_height=256, hr_width=256,\n",
    "                 channels=3, sample_interval=100, checkpoint_interval=100, residual_blocks=23, warmup_batches=500,\n",
    "                 lambda_adv=5e-3, lambda_pixel=1e-2):\n",
    "        os.makedirs(\"images/training\", exist_ok=True)\n",
    "        os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "        # self.epoch = epoch\n",
    "        # self.n_epochs = n_epochs\n",
    "        # self.dataset_name = dataset_name\n",
    "        # self.batch_size = batch_size\n",
    "        # self.lr = lr\n",
    "        # self.b1 = b1\n",
    "        # self.b2 = b2\n",
    "        # self.decay_epoch = decay_epoch\n",
    "        # self.n_cpu = n_cpu\n",
    "        # self.hr_height = hr_height\n",
    "        # self.hr_width = hr_width\n",
    "        # self.channels = channels\n",
    "        # self.sample_interval = sample_interval\n",
    "        # self.checkpoint_interval = checkpoint_interval\n",
    "        # self.residual_blocks = residual_blocks\n",
    "        # self.warmup_batches = warmup_batches\n",
    "        # self.lambda_adv = lambda_adv\n",
    "        # self.lambda_pixel = lambda_pixel\n",
    "\n",
    "        self.reload(b1, b2, batch_size, channels, checkpoint_interval, dataset_name, decay_epoch, epoch, hr_height,\n",
    "                    hr_width, lambda_adv, lambda_pixel, lr, n_cpu, n_epochs, residual_blocks, sample_interval,\n",
    "                    warmup_batches)\n",
    "\n",
    "    def reload(self, b1, b2, batch_size, channels, checkpoint_interval, dataset_name, decay_epoch, epoch, hr_height,\n",
    "               hr_width, lambda_adv, lambda_pixel, lr, n_cpu, n_epochs, residual_blocks, sample_interval,\n",
    "               warmup_batches):\n",
    "        self.parser = argparse.ArgumentParser()\n",
    "        self.opt = self.__commandline_interface(epoch=epoch, n_epochs=n_epochs, dataset_name=dataset_name,\n",
    "                                                batch_size=batch_size, lr=lr, b1=b1, b2=b2, decay_epoch=decay_epoch,\n",
    "                                                n_cpu=n_cpu, hr_height=hr_height, hr_width=hr_width, channels=channels,\n",
    "                                                sample_interval=sample_interval,\n",
    "                                                checkpoint_interval=checkpoint_interval,\n",
    "                                                residual_blocks=residual_blocks, warmup_batches=warmup_batches,\n",
    "                                                lambda_adv=lambda_adv, lambda_pixel=lambda_pixel)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Initialize generator and discriminator\n",
    "        hr_shape = (self.opt.hr_height, self.opt.hr_width)\n",
    "        self.discriminator, self.feature_extractor, self.generator = self.network_initializers(hr_shape)\n",
    "        # Losses\n",
    "        self.criterion_GAN, self.criterion_content, self.criterion_pixel = self.losses()\n",
    "        # Optimizers\n",
    "        self.optimizer_D, self.optimizer_G = self.optimizers()\n",
    "        # Data\n",
    "        self.dataset = ImageDataset_superresolution(device=self.device, root=self.opt.dataset_name, hr_shape=hr_shape)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.opt.batch_size,\n",
    "            num_workers=self.opt.n_cpu,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, args, kwargs):\n",
    "        pass\n",
    "\n",
    "    def __commandline_interface(self, epoch=0, n_epochs=200, dataset_name=\"dataset\", batch_size=4, lr=0.0002, b1=0.9,\n",
    "                                b2=0.999, decay_epoch=100, n_cpu=8, hr_height=256, hr_width=256,\n",
    "                                channels=3, sample_interval=100, checkpoint_interval=100, residual_blocks=23,\n",
    "                                warmup_batches=500, lambda_adv=5e-3, lambda_pixel=1e-2):\n",
    "        self.parser.add_argument(\"--epoch\", type=int, default=epoch, help=\"epoch to start training from\")\n",
    "        self.parser.add_argument(\"--n_epochs\", type=int, default=n_epochs, help=\"number of epochs of training\")\n",
    "        self.parser.add_argument(\"--dataset_name\", type=str, default=dataset_name,\n",
    "                            help=\"name of the dataset\")  # img_align_celeba\n",
    "        self.parser.add_argument(\"--batch_size\", type=int, default=batch_size, help=\"size of the batches\")  # 4\n",
    "        self.parser.add_argument(\"--lr\", type=float, default=lr, help=\"adam: learning rate\")\n",
    "        self.parser.add_argument(\"--b1\", type=float, default=b1, help=\"adam: decay of first order momentum of gradient\")\n",
    "        self.parser.add_argument(\"--b2\", type=float, default=b2, help=\"adam: decay of first order momentum of gradient\")\n",
    "        self.parser.add_argument(\"--decay_epoch\", type=int, default=decay_epoch, help=\"epoch from which to start lr decay\")\n",
    "        self.parser.add_argument(\"--n_cpu\", type=int, default=n_cpu,\n",
    "                            help=\"number of cpu threads to use during batch generation\")\n",
    "        self.parser.add_argument(\"--hr_height\", type=int, default=hr_height, help=\"high res. image height\")\n",
    "        self.parser.add_argument(\"--hr_width\", type=int, default=hr_width, help=\"high res. image width\")\n",
    "        self.parser.add_argument(\"--channels\", type=int, default=channels, help=\"number of image channels\")\n",
    "        self.parser.add_argument(\"--sample_interval\", type=int, default=sample_interval,\n",
    "                            help=\"interval between saving image samples\")\n",
    "        self.parser.add_argument(\"--checkpoint_interval\", type=int, default=checkpoint_interval,\n",
    "                            help=\"batch interval between model checkpoints\")  # 5000\n",
    "        self.parser.add_argument(\"--residual_blocks\", type=int, default=residual_blocks,\n",
    "                            help=\"number of residual blocks in the generator\")\n",
    "        self.parser.add_argument(\"--warmup_batches\", type=int, default=warmup_batches,\n",
    "                            help=\"number of batches with pixel-wise loss only\")\n",
    "        self.parser.add_argument(\"--lambda_adv\", type=float, default=lambda_adv, help=\"adversarial loss weight\")\n",
    "        self.parser.add_argument(\"--lambda_pixel\", type=float, default=lambda_pixel, help=\"pixel-wise loss weight\")\n",
    "        opt = self.parser.parse_args()\n",
    "        print(opt)\n",
    "        return opt\n",
    "\n",
    "    def network_initializers(self, hr_shape, use_LeakyReLU_Mish=False):\n",
    "        generator = GeneratorRRDB(self.opt.channels, filters=64, num_res_blocks=self.opt.residual_blocks,\n",
    "                                  use_LeakyReLU_Mish=use_LeakyReLU_Mish).to(self.device, non_blocking=True)\n",
    "        discriminator = Discriminator(input_shape=(self.opt.channels, *hr_shape),\n",
    "                                      use_LeakyReLU_Mish=use_LeakyReLU_Mish).to(self.device, non_blocking=True)\n",
    "        feature_extractor = FeatureExtractor().to(self.device, non_blocking=True)\n",
    "        # Set feature extractor to inference mode\n",
    "        feature_extractor.eval()\n",
    "        return discriminator, feature_extractor, generator\n",
    "\n",
    "    def losses(self):\n",
    "        criterion_GAN = torch.nn.BCEWithLogitsLoss()\n",
    "        criterion_content = torch.nn.L1Loss()\n",
    "        criterion_pixel = torch.nn.L1Loss()\n",
    "        return criterion_GAN, criterion_content, criterion_pixel\n",
    "\n",
    "    def optimizers(self):\n",
    "        optimizer_G = torch.optim.Adam(self.generator.parameters(), lr=self.opt.lr, betas=(self.opt.b1, self.opt.b2))\n",
    "        optimizer_D = torch.optim.Adam(self.discriminator.parameters(), lr=self.opt.lr,\n",
    "                                       betas=(self.opt.b1, self.opt.b2))\n",
    "        return optimizer_D, optimizer_G\n",
    "\n",
    "    def __train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39577d52-b273-4cc8-b06e-40fd972c08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset_superresolution(Dataset):\n",
    "    def __init__(self, device, root=None, hr_shape=None, interpolation='bicubic', load_all=True):\n",
    "        hr_height, hr_width = hr_shape\n",
    "        self.hr_height = hr_height\n",
    "        self.hr_width = hr_width\n",
    "        self.device = device\n",
    "        self.load_all = load_all\n",
    "\n",
    "        # ## kornia\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        # self.lr_transform = kornia.nn.Sequential(\n",
    "        #     kornia.geometry.Resize(size=(hr_height // 4, hr_height // 4), interpolation=interpolation),\n",
    "        #     kornia.color.Normalize(torch.from_numpy(mean), torch.from_numpy(std)),\n",
    "        # ).to(device=device)\n",
    "        # self.hr_transform = kornia.nn.Sequential(\n",
    "        #     kornia.geometry.Resize(size=(hr_height, hr_height), interpolation=interpolation),\n",
    "        #     kornia.color.Normalize(torch.from_numpy(mean), torch.from_numpy(std)),\n",
    "        # )\n",
    "\n",
    "        # ## torchvision\n",
    "        # self.lr_transform = transforms.Compose(\n",
    "        #     [\n",
    "        # #       transforms.ToPILImage(),\n",
    "        #         transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize(mean, std),\n",
    "        #     ]\n",
    "        # )\n",
    "        # self.hr_transform = transforms.Compose(\n",
    "        #     [\n",
    "        # #       transforms.ToPILImage(),\n",
    "        #         transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize(mean, std),\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "        ## Albumentation\n",
    "        self.lr_transform = Compose([\n",
    "            Resize(hr_height // 4, hr_height // 4),\n",
    "            Normalize(mean, std),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.hr_transform = Compose([\n",
    "            Resize(hr_height, hr_height),\n",
    "            Normalize(mean, std),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "\n",
    "        if isinstance(root, str):\n",
    "            self.files = sorted(glob.glob(root + \"/*.*\"))\n",
    "        else:\n",
    "            self.files = root\n",
    "\n",
    "        if load_all:\n",
    "            self.images_lr, self.images_hr = self.__load_all_images()\n",
    "\n",
    "    def __load_all_images(self):\n",
    "        files_len = len(self.files)\n",
    "\n",
    "        index = 0\n",
    "        img_pivot = cv2.imread(self.files[index % files_len])\n",
    "        img_lr = self.lr_transform(image=img_pivot)['image']\n",
    "        img_hr = self.hr_transform(image=img_pivot)['image']\n",
    "        images_lr = torch.zeros((files_len, img_pivot.shape[-1], self.hr_height//4, self.hr_width//4))\n",
    "        images_hr = torch.zeros((files_len, img_pivot.shape[-1], self.hr_height, self.hr_width))\n",
    "\n",
    "        images_lr[index, ...] = img_lr\n",
    "        images_hr[index, ...] = img_hr\n",
    "        for index in range(1, files_len):\n",
    "            img = cv2.imread(self.files[index % files_len])\n",
    "            img_lr = self.lr_transform(image=img)['image']\n",
    "            img_hr = self.hr_transform(image=img)['image']\n",
    "            images_lr[index, ...] = img_lr\n",
    "            images_hr[index, ...] = img_hr\n",
    "        return images_lr, images_hr\n",
    "\n",
    "    def load_one_image(self, index):\n",
    "        if isinstance(self.files[index], str):\n",
    "            img = cv2.imread(self.files[index % len(self.files)])\n",
    "        else:\n",
    "            img = self.files\n",
    "        img_lr = self.lr_transform(image=img)['image']\n",
    "        img_hr = self.hr_transform(image=img)['image']\n",
    "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.load_all:\n",
    "            return {\"lr\": self.images_lr[index], \"hr\": self.images_hr[index]}\n",
    "        else:\n",
    "            return self.load_one_image(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_name = \"../TurkishPlates\"\n",
    "    hr_shape = (256, 256)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = ImageDataset_superresolution(device=device, root=dataset_name, hr_shape=hr_shape)\n",
    "    print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba1897-b908-4e2f-9fa0-4ae39bbadeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class esrgan(train_loop):\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.esrgan_train()\n",
    "\n",
    "    def esrgan_train(self):\n",
    "        if self.opt.epoch != 0:\n",
    "            # Load pretrained models\n",
    "            self.generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\" % self.opt.epoch))\n",
    "            self.discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\" % self.opt.epoch))\n",
    "        self.__train()\n",
    "\n",
    "    def __train(self):\n",
    "        for epoch in range(self.opt.epoch, self.opt.n_epochs):\n",
    "            for i, imgs in enumerate(self.dataloader):\n",
    "\n",
    "                batches_done = epoch * len(self.dataloader) + i\n",
    "\n",
    "                # Configure model input\n",
    "                imgs_lr = imgs[\"lr\"].to(self.device, non_blocking=True)\n",
    "                imgs_hr = imgs[\"hr\"].to(self.device, non_blocking=True)\n",
    "\n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones((imgs_lr.size(0), *self.discriminator.output_shape), requires_grad=False).to(\n",
    "                    self.device, non_blocking=True)\n",
    "                fake = torch.zeros((imgs_lr.size(0), *self.discriminator.output_shape), requires_grad=False).to(\n",
    "                    self.device, non_blocking=True)\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                self.optimizer_G.zero_grad()\n",
    "\n",
    "                # Generate a high resolution image from low resolution input\n",
    "                gen_hr = self.generator(imgs_lr)\n",
    "\n",
    "                # Measure pixel-wise loss against ground truth\n",
    "                loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)  # L1Loss\n",
    "\n",
    "                if batches_done < self.opt.warmup_batches:\n",
    "                    # Warm-up (pixel-wise loss only)\n",
    "                    loss_pixel.backward()\n",
    "                    self.optimizer_G.step()\n",
    "                    print(\n",
    "                        \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n",
    "                        % (epoch, self.opt.n_epochs, i, len(self.dataloader), loss_pixel.item())\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Extract validity predictions from discriminator\n",
    "                pred_real = self.discriminator(imgs_hr).detach()\n",
    "                pred_fake = self.discriminator(gen_hr)\n",
    "\n",
    "                # Adversarial loss (relativistic average GAN)\n",
    "                loss_GAN = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "                # Content loss\n",
    "                real_features = self.feature_extractor(imgs_hr).detach()\n",
    "                gen_features = self.feature_extractor(gen_hr)\n",
    "                loss_content = self.criterion_content(gen_features, real_features)  # L1Loss\n",
    "\n",
    "                # Total generator loss\n",
    "                loss_G = loss_content + self.opt.lambda_adv * loss_GAN + self.opt.lambda_pixel * loss_pixel\n",
    "\n",
    "                loss_G.backward()\n",
    "                self.optimizer_G.step()\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                loss_D = self.train_discriminator(gen_hr, imgs_hr, fake, valid)\n",
    "\n",
    "                # --------------\n",
    "                #  Log Progress\n",
    "                # --------------\n",
    "\n",
    "                self.__log_progress(i, batches_done, epoch, gen_hr, imgs_lr, loss_D, loss_G, loss_GAN, loss_content,\n",
    "                                    loss_pixel)\n",
    "\n",
    "    def train_discriminator(self, gen_hr, imgs_hr, fake, valid):\n",
    "        self.optimizer_D.zero_grad()\n",
    "        pred_real = self.discriminator(imgs_hr)\n",
    "        pred_fake = self.discriminator(gen_hr.detach())\n",
    "        # Adversarial loss for real and fake images (relativistic average GAN)\n",
    "        loss_real = self.criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "        # Total loss\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        loss_D.backward()\n",
    "        self.optimizer_D.step()\n",
    "        return loss_D\n",
    "\n",
    "    def __log_progress(self, i, batches_done, epoch, gen_hr, imgs_lr, loss_D, loss_G, loss_GAN, loss_content,\n",
    "                       loss_pixel):\n",
    "        self.summary_string = \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, content: %f, adv: %f, pixel: %f]\"\\\n",
    "            % (\n",
    "                epoch,\n",
    "                self.opt.n_epochs,\n",
    "                i,\n",
    "                len(self.dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_content.item(),\n",
    "                loss_GAN.item(),\n",
    "                loss_pixel.item(),\n",
    "            )\n",
    "        print(self.summary_string)\n",
    "        if batches_done % self.opt.sample_interval == 0:\n",
    "            # Save image grid with upsampled inputs and ESRGAN outputs\n",
    "            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
    "            img_grid = denormalize(torch.cat((imgs_lr, gen_hr), -1))\n",
    "            save_image(img_grid, \"images/training/%d.png\" % batches_done, nrow=1, normalize=False)\n",
    "        if batches_done % self.opt.checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(self.generator.state_dict(), \"saved_models/generator_%d.pth\" % epoch)\n",
    "            torch.save(self.discriminator.state_dict(), \"saved_models/discriminator_%d.pth\" % epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811c6ae-4ec4-4f78-a1a2-1c21ae575891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensors):\n",
    "    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n",
    "    for c in range(3):\n",
    "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
    "    return torch.clamp(tensors, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d99b5c-76e2-451f-a42d-5cf76fc65e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2838f-4006-4917-b3a8-c6164bc7e202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd09f0-6429-4dd5-a807-c6f24e731d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9d5b1-318c-459a-8b81-57d251b1e023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453f016-c3c7-4259-97eb-4070b37629bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "superres = esrgan()\n",
    "superres()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
